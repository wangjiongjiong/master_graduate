import sys
import os
import torch
import numpy as np
import einops
from PIL import Image
from tqdm import tqdm
from omegaconf import OmegaConf
from openai import OpenAI  # å¿…é¡»å®‰è£…: pip install openai

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®ä¸­çš„æ¨¡å—
if './' not in sys.path:
    sys.path.append('./')

from ldm.util import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler
from torch.utils.data import DataLoader
# å‡è®¾æ‚¨çš„ dataset æ–‡ä»¶åœ¨ src/datasets/dataset_display.py
from src.datasets.dataset_display import MyDataset 

# ==============================================================================
# âš™ï¸ [é…ç½®åŒºåŸŸ] è¯·åœ¨è¿™é‡Œä¿®æ”¹æ‚¨çš„è·¯å¾„å’Œ Key
# ==============================================================================

# 1. è·¯å¾„è®¾ç½®
LAYOUT_TXT_PATH = 'demo/txttest'   # æ‚¨çš„ txt å¸ƒå±€æ–‡ä»¶å¤¹è·¯å¾„
CKPT_PATH = './ckpt/last1.ckpt'             # æ‚¨çš„æ¨¡å‹æƒé‡è·¯å¾„
#CKPT_PATH = './ckpt/aerogen_diorr_last.ckpt' 
CONFIG_PATH = 'configs/stable-diffusion/dual/v1-finetune-DIOR-R.yaml' # é…ç½®æ–‡ä»¶è·¯å¾„
OUTPUT_DIR = './demo/output_llm_enhanced'   # ç»“æœä¿å­˜è·¯å¾„

# 2. LLM è®¾ç½® (å¤§æ¨¡å‹ API)
# æ¨èä½¿ç”¨ DeepSeek (ä¾¿å®œä¸”å¼º) æˆ– OpenAI
API_KEY = "sk-0b6123d9da0a4a2ab04eac5b3d3cf04f"  # ğŸ”´ è¯·æ›¿æ¢ä¸ºæ‚¨çš„ API Key
API_BASE_URL = "https://api.deepseek.com"      # å¦‚æœç”¨ OpenAIï¼Œè¯·åˆ æ‰è¿™è¡Œæˆ–ç•™ç©º
API_MODEL_NAME = "deepseek-chat"               # æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "gpt-4o" æˆ– "deepseek-chat"
ENABLE_LLM = True                              # å¦‚æœä¸æƒ³ç”¨ LLMï¼Œæ”¹ä¸º Falseï¼Œå°†ä½¿ç”¨è§„åˆ™æ‹¼æ¥

# 3. ç”Ÿæˆè®¾ç½®
RESOLUTION = 512
BATCH_SIZE = 1        # å»ºè®®ä¸º 1ï¼Œå› ä¸º LLM ç”Ÿæˆ Prompt éœ€è¦æ—¶é—´
NUM_SAMPLES = 1      # æ¯å¼ å¸ƒå±€ç”Ÿæˆå‡ å¼ å›¾ (FID æµ‹è¯•å»ºè®® 1)
DDIM_STEPS = 50       # é‡‡æ ·æ­¥æ•°
GUIDANCE_SCALE = 7.5  # CFG Scale (è¶Šå¤§è¶Šå¬è¯ï¼Œè¶Šå°è¶Šè‡ªç„¶)

# DIOR ç±»åˆ«æ˜ å°„ (0-19)
ID_TO_CLASS = {
    0: "airplane", 1: "airport", 2: "baseballfield", 3: "basketballcourt",
    4: "bridge", 5: "chimney", 6: "dam", 7: "Expressway-Service-area",
    8: "Expressway-toll-station", 9: "golffield", 10: "groundtrackfield",
    11: "harbor", 12: "overpass", 13: "ship", 14: "stadium",
    15: "storagetank", 16: "tenniscourt", 17: "trainstation",
    18: "vehicle", 19: "windmill"
}

# ==============================================================================
# ğŸ§  [æ ¸å¿ƒé€»è¾‘] Prompt ç”Ÿæˆæ¨¡å—
# ==============================================================================

# åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
client = None
if ENABLE_LLM and "sk-" in API_KEY:
    try:
        client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)
        print("âœ… LLM Client åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ LLM åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å¤‡ç”¨è§„åˆ™æ¨¡å¼ã€‚")

def get_objects_from_txt_file(txt_path):
    """è¯»å– txt æ–‡ä»¶ï¼Œåˆ†æé‡Œé¢æœ‰ä»€ä¹ˆç‰©ä½“"""
    if not os.path.exists(txt_path):
        return []
    
    objects = []
    with open(txt_path, 'r') as f:
        for line in f:
            if not line.strip(): continue
            try:
                parts = line.strip().split()
                class_id = int(parts[0])
                class_name = ID_TO_CLASS.get(class_id, "object")
                objects.append(class_name)
            except:
                continue
    return objects

def generate_enhanced_prompt(filename, original_txt_path):
    """
    è¾“å…¥: æ–‡ä»¶å (ä¾‹å¦‚ '00001.txt') å’Œ å®Œæ•´è·¯å¾„
    è¾“å‡º: å¢å¼ºåçš„ Prompt
    """
    full_path = os.path.join(original_txt_path, filename)
    object_list = get_objects_from_txt_file(full_path)
    
    # ç»Ÿè®¡ç‰©ä½“æ•°é‡ï¼Œä¾‹å¦‚: "2 airplanes, 1 groundtrackfield"
    from collections import Counter
    counts = Counter(object_list)
    scene_desc = ", ".join([f"{v} {k}(s)" for k, v in counts.items()])
    
    if not scene_desc:
        scene_desc = "various objects"

    # --- æ¨¡å¼ A: ä½¿ç”¨ LLM ç”Ÿæˆ (é«˜è´¨é‡) ---
    if ENABLE_LLM and client:
        system_prompt = """
        You are an expert in writing prompts for AI satellite image generation.
        Input: A list of objects in the scene.
        Output: A concise, photorealistic, high-quality caption for Stable Diffusion.
        
        Rules:
        1. Start with "A high-resolution top-down satellite image of..."
        2. Describe the background texture (e.g., concrete for planes, blue water for ships, red track for sports).
        3. Add quality keywords: "8k, sharp focus, cinematic lighting, detailed shadows, hdr".
        4. Keep it under 60 words. No explanations.
        """
        user_content = f"Scene objects: {scene_desc}."
        
        try:
            response = client.chat.completions.create(
                model=API_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                temperature=0.7,
                max_tokens=100
            )
            llm_prompt = response.choices[0].message.content.strip()
            # print(f"ğŸ¤– LLM Prompt: {llm_prompt}") 
            return llm_prompt
        except Exception as e:
            print(f"âš ï¸ LLM è°ƒç”¨å‡ºé”™: {e}ï¼Œåˆ‡æ¢å›è§„åˆ™æ¨¡å¼ã€‚")

    # --- æ¨¡å¼ B: è§„åˆ™æ‹¼æ¥ (å¤‡ç”¨/å¿«é€Ÿ) ---
    # å¦‚æœæ²¡å¼€ LLM æˆ–è€…è°ƒç”¨å¤±è´¥ï¼Œç”¨è¿™ä¸ª
    base = f"A professional high-resolution optical satellite imagery, top-down view of {scene_desc}. "
    
    details = ""
    if "airplane" in scene_desc: details += "Parked on grey concrete apron with markings. "
    elif "ship" in scene_desc: details += "Docked in deep blue water with waves. "
    elif "groundtrackfield" in scene_desc: details += "Red running track with green grass field. "
    elif "tenniscourt" in scene_desc: details += "Blue and green hard court surfaces. "
    elif "storagetank" in scene_desc: details += "White industrial tanks structure. "
    
    suffix = "Highly detailed, 4k resolution, sharp focus, cinematic lighting, realistic textures, clear shadows."
    return base + details + suffix

# ==============================================================================
# ğŸš€ [ä¸»ç¨‹åº] æ¨ç†å¾ªç¯
# ==============================================================================

def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd)
    if len(m) > 0 and verbose:
        print("missing keys:", m)
    if len(u) > 0 and verbose:
        print("unexpected keys:", u)
    model.cuda()
    model.eval()
    return model

def main():
    # 1. å‡†å¤‡æ¨¡å‹
    config = OmegaConf.load(CONFIG_PATH)
    model = load_model_from_config(config, CKPT_PATH)
    sampler = DDIMSampler(model)

    # 2. å‡†å¤‡æ•°æ®
    dataset = MyDataset(LAYOUT_TXT_PATH, RESOLUTION, mask_size=64) # æ³¨æ„ mask_size éœ€æ ¹æ®è®­ç»ƒé…ç½®è°ƒæ•´
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # Windowsä¸‹å»ºè®®è®¾0

    print(f"\nğŸš€ å¼€å§‹æ¨ç†...")
    print(f"ğŸ“‚ è¾“å…¥è·¯å¾„: {LAYOUT_TXT_PATH}")
    print(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {OUTPUT_DIR}")
    print(f"ğŸ¤– LLM å¯ç”¨çŠ¶æ€: {ENABLE_LLM}")

    # 3. å¾ªç¯ç”Ÿæˆ
    for batch in tqdm(dataloader, desc="Generating"):
        filenames = batch['filename']
        
        # --- [å…³é”®æ­¥éª¤] ç”Ÿæˆå¢å¼ºçš„ Prompts ---
        refined_prompts = []
        for fname in filenames:
            # ä¼ å…¥æ–‡ä»¶åï¼Œå» txt é‡ŒæŸ¥ç‰©ä½“ï¼Œç„¶åè®© LLM å†™ prompt
            p = generate_enhanced_prompt(fname, LAYOUT_TXT_PATH)
            refined_prompts.append(p)
        
        # æ‰¹é‡å¤åˆ¶ Prompt (å¦‚æœ num_samples > 1)
        # ä¾‹å¦‚: ["prompt1"] -> ["prompt1", "prompt1"]
        c_prompts = []
        for p in refined_prompts:
            c_prompts.extend([p] * NUM_SAMPLES)

        with torch.no_grad():
            # å‡†å¤‡ Condition
            mask_controls = batch['mask_conditions'].float().cuda()
            category_controls = batch['category_conditions'].float().cuda()
            bbox_controls = batch['bboxes'].float().cuda()
            mask_vectors = batch['mask_vector'].float().cuda()
            
            # å †å  Condition (æ”¯æŒ num_samples)
            c_bbox = torch.cat([bbox_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_cat = torch.cat([category_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_mask = torch.cat([mask_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_vec = torch.cat([mask_vectors for _ in range(NUM_SAMPLES)], dim=0)

            cond = {
                "c_crossattn": [model.get_learned_conditioning(c_prompts)],
                "bbox_control": [c_bbox],
                "category_control": [c_cat],
                "mask_control": [c_mask],
                "mask_vector": [c_vec],
            }
            
            # --- [å…³é”®æ­¥éª¤] è´Ÿé¢æç¤ºè¯ (Negative Prompt) ---
            neg_prompt_text = "low quality, blur, pixelated, distortion, lowres, bad anatomy, text, watermark, foggy, haze, cartoon, painting, illustration"
            un_cond = {
                "c_crossattn": [model.get_learned_conditioning([neg_prompt_text] * len(c_prompts))],
                "bbox_control": [c_bbox],
                "category_control": [c_cat],
                "mask_control": [c_mask],
                "mask_vector": [c_vec],
            }

            # é‡‡æ ·
            shape = (4, RESOLUTION // 8, RESOLUTION // 8)
            samples, _ = sampler.sample(
                S=DDIM_STEPS,
                conditioning=cond,
                batch_size=len(c_prompts),
                shape=shape,
                verbose=False,
                unconditional_guidance_scale=GUIDANCE_SCALE,
                unconditional_conditioning=un_cond,
                eta=0.2 # å¢åŠ ä¸€ç‚¹éšæœºæ€§
            )

            # è§£ç  VAE
            x_samples = model.decode_first_stage(samples)
            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
            
            # ä¿å­˜å›¾ç‰‡
            for i, filename in enumerate(filenames):
                # æ¯ä¸ª layout å¯èƒ½ç”Ÿæˆå¤šå¼ 
                for j in range(NUM_SAMPLES):
                    img_tensor = x_samples[i * NUM_SAMPLES + j]
                    img_np = 255. * einops.rearrange(img_tensor, 'c h w -> h w c').cpu().numpy()
                    img_pil = Image.fromarray(img_np.astype(np.uint8))
                    
                    # ä¿å­˜ç»“æ„: output_dir/00001.jpg
                    # å¦‚æœ num_samples > 1, å¯ä»¥åœ¨æ–‡ä»¶ååŠ åç¼€
                    save_name = filename.replace('.txt', '.jpg').replace('txt', 'jpg')
                    if NUM_SAMPLES > 1:
                        save_name = f"{filename.split('.')[0]}_{j}.jpg"
                    
                    os.makedirs(OUTPUT_DIR, exist_ok=True)
                    save_path = os.path.join(OUTPUT_DIR, save_name)
                    img_pil.save(save_path)
                    
                    # å¦‚æœæƒ³ä¿å­˜å¯¹åº”çš„ Prompt æ–¹ä¾¿æ£€æŸ¥
                    with open(save_path.replace('.jpg', '.txt'), 'w') as f:
                        f.write(refined_prompts[i])

    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()