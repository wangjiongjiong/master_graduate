import sys
import os
import torch
import numpy as np
import einops
from PIL import Image
from tqdm import tqdm
from omegaconf import OmegaConf
from openai import OpenAI  # å¿…é¡»å®‰è£…: pip install openai

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®ä¸­çš„æ¨¡å—
if './' not in sys.path:
    sys.path.append('./')

from ldm.util import instantiate_from_config
from ldm.models.diffusion.ddim import DDIMSampler
from torch.utils.data import DataLoader
# å‡è®¾æ‚¨çš„ dataset æ–‡ä»¶åœ¨ src/datasets/dataset_display.py
from src.datasets.dataset_display import MyDataset 

# ==============================================================================
# âš™ï¸ [é…ç½®åŒºåŸŸ]
# ==============================================================================

# 1. è·¯å¾„è®¾ç½®
LAYOUT_TXT_PATH = 'demo/txttest'        # æ‚¨çš„ txt å¸ƒå±€æ–‡ä»¶å¤¹è·¯å¾„
CKPT_PATH = './ckpt/last1.ckpt'     # æ‚¨çš„æ¨¡å‹æƒé‡è·¯å¾„
CONFIG_PATH = 'configs/stable-diffusion/dual/v1-finetune-DIOR-R.yaml' # é…ç½®æ–‡ä»¶è·¯å¾„
OUTPUT_DIR = './demo/output_llm_clean2'   # ç»“æœä¿å­˜è·¯å¾„

# 2. LLM è®¾ç½® (å¤§æ¨¡å‹ API)
API_KEY = "sk-0b6123d9da0a4a2ab04eac5b3d3cf04f"      # ğŸ”´ è¯·æ›¿æ¢ä¸ºæ‚¨çš„ API Key
API_BASE_URL = "https://api.deepseek.com"    # DeepSeek æˆ– OpenAI åœ°å€
API_MODEL_NAME = "deepseek-chat"             
ENABLE_LLM = True                            

# 3. ç”Ÿæˆè®¾ç½®
RESOLUTION = 512
BATCH_SIZE = 1       
NUM_SAMPLES = 1      
DDIM_STEPS = 50      
GUIDANCE_SCALE = 7.5 

# DIOR ç±»åˆ«æ˜ å°„
ID_TO_CLASS = {
    0: "airplane", 1: "airport", 2: "baseballfield", 3: "basketballcourt",
    4: "bridge", 5: "chimney", 6: "dam", 7: "Expressway-Service-area",
    8: "Expressway-toll-station", 9: "golffield", 10: "groundtrackfield",
    11: "harbor", 12: "overpass", 13: "ship", 14: "stadium",
    15: "storagetank", 16: "tenniscourt", 17: "trainstation",
    18: "vehicle", 19: "windmill"
}

# ==============================================================================
# ğŸ§  [æ ¸å¿ƒé€»è¾‘] æç®€ç‰ˆ LLM Prompt ç”Ÿæˆ
# ==============================================================================

client = None
if ENABLE_LLM and "sk-" in API_KEY:
    try:
        client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)
        print("âœ… LLM Client åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ LLM åˆå§‹åŒ–å¤±è´¥: {e}")

def get_objects_from_txt_file(txt_path):
    """è¯»å– txt æ–‡ä»¶ï¼Œè¿”å›ç‰©ä½“åˆ—è¡¨"""
    if not os.path.exists(txt_path):
        return []
    objects = []
    with open(txt_path, 'r') as f:
        for line in f:
            if not line.strip(): continue
            try:
                parts = line.strip().split()
                class_id = int(parts[0])
                class_name = ID_TO_CLASS.get(class_id, "object")
                objects.append(class_name)
            except:
                continue
    return objects

def generate_clean_prompt(filename, original_txt_path):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆæç®€ã€å‡†ç¡®çš„æè¿°
    """
    full_path = os.path.join(original_txt_path, filename)
    object_list = get_objects_from_txt_file(full_path)
    
    # ç»Ÿè®¡ç‰©ä½“ (ä¾‹å¦‚: 2 airplanes, 1 airport)
    from collections import Counter
    counts = Counter(object_list)
    scene_desc = ", ".join([f"{v} {k}" for k, v in counts.items()])
    
    if not scene_desc:
        scene_desc = "background only"

    # --- å…³é”®ä¿®æ”¹ï¼šSystem Prompt ---
    # å¼ºåˆ¶ LLM åªåšâ€œç»Ÿè®¡å‘˜â€å’Œâ€œèƒŒæ™¯æ¨ç†å‘˜â€ï¼Œç¦æ­¢åšâ€œæ–‡å­¦å®¶â€
    if ENABLE_LLM and client:
        system_prompt = """
        You are a strict data formatter for satellite images.
        Input: A list of objects and their counts.
        Task: 
        1. Identify the most logical background context (e.g., 'airport' for airplanes, 'harbor' for ships).
        2. Generate a simple sentence.
        
        Strict Constraints:
        - Format MUST be: "An aerial image containing [Quantity] [Objects] with [Context] background."
        - DO NOT use adjectives like 'detailed', 'cinematic', '4k', 'sharp', 'beautiful'.
        - DO NOT add any extra explanation.
        """
        
        user_content = f"Objects found: {scene_desc}."
        
        try:
            response = client.chat.completions.create(
                model=API_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                temperature=0.1, # ä½æ¸©åº¦ï¼Œä¿è¯å›ç­”ç¨³å®šæ­»æ¿
                max_tokens=50
            )
            llm_prompt = response.choices[0].message.content.strip()
            # print(f"ğŸ¤– LLM Clean Prompt: {llm_prompt}") 
            return llm_prompt
        except Exception as e:
            print(f"âš ï¸ LLM å‡ºé”™: {e}ï¼Œä½¿ç”¨å¤‡ç”¨è§„åˆ™ã€‚")

    # --- å¤‡ç”¨è§„åˆ™ (å¦‚æœ LLM æŒ‚äº†) ---
    return f"An aerial image containing {scene_desc}."

# ==============================================================================
# ğŸš€ [ä¸»ç¨‹åº]
# ==============================================================================

def load_model_from_config(config, ckpt):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    model.load_state_dict(sd, strict=False)
    model.cuda()
    model.eval()
    return model

def main():
    config = OmegaConf.load(CONFIG_PATH)
    model = load_model_from_config(config, CKPT_PATH)
    sampler = DDIMSampler(model)

    dataset = MyDataset(LAYOUT_TXT_PATH, RESOLUTION, mask_size=64) 
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    print(f"\nğŸš€ å¼€å§‹æ¨ç† (LLM æç®€æ¨¡å¼)...")
    
    for batch in tqdm(dataloader, desc="Generating"):
        filenames = batch['filename']
        
        # 1. LLM ç”Ÿæˆ Prompt
        refined_prompts = []
        for fname in filenames:
            p = generate_clean_prompt(fname, LAYOUT_TXT_PATH)
            refined_prompts.append(p)
            
        c_prompts = []
        for p in refined_prompts:
            c_prompts.extend([p] * NUM_SAMPLES)

        with torch.no_grad():
            mask_controls = batch['mask_conditions'].float().cuda()
            category_controls = batch['category_conditions'].float().cuda()
            bbox_controls = batch['bboxes'].float().cuda()
            mask_vectors = batch['mask_vector'].float().cuda()
            
            c_bbox = torch.cat([bbox_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_cat = torch.cat([category_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_mask = torch.cat([mask_controls for _ in range(NUM_SAMPLES)], dim=0)
            c_vec = torch.cat([mask_vectors for _ in range(NUM_SAMPLES)], dim=0)

            cond = {
                "c_crossattn": [model.get_learned_conditioning(c_prompts)],
                "bbox_control": [c_bbox],
                "category_control": [c_cat],
                "mask_control": [c_mask],
                "mask_vector": [c_vec],
            }
            
            # è´Ÿé¢æç¤ºè¯ï¼šä¾ç„¶ä¿ç•™ï¼Œä¿è¯ç”»é¢å¹²å‡€ï¼Œä½†å»æ‰äº†é£æ ¼åŒ–çš„è´Ÿé¢è¯
            neg_text = "low quality, blur, pixelated, distortion, lowres, bad anatomy, text, watermark, foggy"
            un_cond = {
                "c_crossattn": [model.get_learned_conditioning([neg_text] * len(c_prompts))],
                "bbox_control": [c_bbox],
                "category_control": [c_cat],
                "mask_control": [c_mask],
                "mask_vector": [c_vec],
            }

            shape = (4, RESOLUTION // 8, RESOLUTION // 8)
            samples, _ = sampler.sample(
                S=DDIM_STEPS,
                conditioning=cond,
                batch_size=len(c_prompts),
                shape=shape,
                verbose=False,
                unconditional_guidance_scale=GUIDANCE_SCALE,
                unconditional_conditioning=un_cond,
                eta=0.0
            )

            x_samples = model.decode_first_stage(samples)
            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
            
            for i, filename in enumerate(filenames):
                for j in range(NUM_SAMPLES):
                    img_tensor = x_samples[i * NUM_SAMPLES + j]
                    img_np = 255. * einops.rearrange(img_tensor, 'c h w -> h w c').cpu().numpy()
                    img_pil = Image.fromarray(img_np.astype(np.uint8))
                    
                    base_name = filename.replace('.txt', '').replace('txt', '')
                    save_name = f"{base_name}.jpg" if NUM_SAMPLES == 1 else f"{base_name}_{j}.jpg"
                    
                    os.makedirs(OUTPUT_DIR, exist_ok=True)
                    save_path = os.path.join(OUTPUT_DIR, save_name)
                    img_pil.save(save_path)
                    
                    # (å»ºè®®) æŠŠ Prompt ä¿å­˜ä¸‹æ¥çœ‹çœ‹ LLM å¬ä¸å¬è¯
                    with open(save_path.replace('.jpg', '.txt'), 'w') as f:
                        f.write(refined_prompts[i])

    print("\nâœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()