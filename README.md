# [CVPR 25] AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation

<a href='https://arxiv.org/abs/2411.15497'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>  <a href=#citation><img src='https://img.shields.io/badge/Paper-BibTex-Green'></a> 
<a href='https://openaccess.thecvf.com/content/CVPR2025/html/Tang_AeroGen_Enhancing_Remote_Sensing_Object_Detection_with_Diffusion-Driven_Data_Generation_CVPR_2025_paper.html'><img src='https://img.shields.io/badge/Paper-CVPR-yellow'></a>
<a href='https://huggingface.co/Sonetto702/AeroGen'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-AeroGen_Model-blue'></a>


- **AeroGen** AeroGen is the first model to simultaneously support horizontal and rotated bounding box condition generation, thus enabling the generation of high-quality synthetic images that meet specific layout and object category requirements.

<div align=center>
<img src="imgs/display.png" height="100%" width="100%"/>
</div>

## ğŸ—“ï¸ TODOs

- [x] Release pretrained models.
- [x] Release inference code.
- [x] Release training code
- [ ] Release Gradio UI.

## ğŸš€ Getting Started

### Conda environment setup
prepare the environment

```bash
conda env create -f environment.yaml
conda activate aerogen
```
You can download pre-trained models from this [huggingface url](https://huggingface.co/Sonetto702/AeroGen) and put it to `./ckpt/` folder.

### âš¡ï¸Quick Generation

You can the following code to generate images more quickly by:
```bash
python src/inference/inference.py
```
You can find the relevant layout files for the presentation in `./demo/` where you can find the relevant layout files for the display.
The following is the example of the generated image.
<div align=center>
<img src="imgs/display1.png" height="80%" width="80%"/>
</div>

## Training Datasets Preperation
We use the DIOR-R dataset as an example to show how to set training dataset.
Download DIOR-R dataset from [url](https://gcheng-nwpu.github.io/) and save in `./datasets/`. 
```
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ DIOR-VOC
â”‚   â”‚   â”œâ”€â”€ Annotations
â”‚   â”‚   â”‚   â”œâ”€â”€ Oriented_Bounding_Boxes
â”‚   â”‚   â”‚       â”œâ”€â”€ ... (annotation files, e.g., .xml)
â”‚   â”‚   â”œâ”€â”€ VOC2007
â”‚   â”‚   â”‚   â”œâ”€â”€ JPEGImages
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ... (image files, e.g., .jpg, .png)
â”‚   â”‚   â”‚   â”œâ”€â”€ ImageSets
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Main
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ train.txt
â”‚   â”œâ”€â”€ category_embeddings.npy
```


## ğŸ¶ Model Training

The following demonstrates the model training process under the DIOR-R dataset, firstly preparing the pytorch environment and the training dataset in [DATASETS](datasets/README.md), then downloading the SD weights fine-tuned on remote sensing images to the ckpt folder at this [url](https://huggingface.co/Sonetto702/AeroGen) & put it to `./ckpt/`, and finally executing the following commands in sequence:

```bash
conda activate aerogen
python src/train/prepare_weight_r.py
bash configs/stable-diffusion/dual/train_r.sh
```
The more information and options an find in `./main.py` and `./configs/stable-diffusion/dual/v1-finetune-DIOR-R.yaml`

## ğŸ“¡ Contact
If you have any questions about the paper or the code, feel free to email me at [aryswph@gmail.com](mailto:aryswph@gmail.com). This ensures I can promptly notice and respond!

## ğŸ’• Acknowledgments:
This repo is built upon [Stable Diffusion](https://github.com/CompVis/stable-diffusion/tree/main), [ControlNet](https://github.com/lllyasviel/ControlNet/tree/main), [CLIP](https://github.com/openai/CLIP), [GLIGEN](https://github.com/gligen/GLIGEN/tree/master). Sincere thanks to their excellent work!


## Citation
```
@article{tang2024aerogen,
  title={AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation},
  author={Tang, Datao and Cao, Xiangyong and Wu, Xuan and Li, Jialin and Yao, Jing and Bai, Xueru and Meng, Deyu},
  journal={arXiv preprint arXiv:2411.15497},
  year={2024}
}
```
